// begin header
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]
:numbered:
:toc: macro
:toc-title: pass:[<b>Table of Contents</b>]
// end header
= RHOC Connectors Availability / SLO Error budget burn

toc::[]

== Description

The purpose of this SOP is to outline the process of resolving the alerts `RHOCConnectorsAvailability*`. The rules for this alert operate within an OSD cluster. There are four error budget burn alerts that are used to protect the RHOC Connectors availability SLO.

* slo_connector_availability_failure_rate:ratio_rate1h
* slo_connector_availability_failure_rate:ratio_rate6h
* slo_connector_availability_failure_rate:ratio_rate24h
* slo_connector_availability_failure_rate:ratio_rate3d

The source of the alert is the
`cos_fleetshard_sync_connector_state_count_total` metric,
and fires when it's error rate is high over a short window of time and a
longer one, indicating that the error budget is being burned at an unsafe rate.

The state of a connector determines the availability. A connector have 5 states:

- CONNECTOR_STATE_READY
- CONNECTOR_STATE_FAILED
- CONNECTOR_STATE_DELETED
- CONNECTOR_STATE_STOPPED
- CONNECTOR_STATE_IN_PROCESS

These vales are recorded in the label `cos_connector_state` which is part of `cos_fleetshard_sync_connector_state_count_total` metric.
The error rate is calculated using  CONNECTOR_STATE_READY and CONNECTOR_STATE_FAILED. In order for this alert to trigger,
the failing connector must reach CONNECTOR_STATE_READY before failing meaning the connector was able to run in the past.

[NOTE]
`cos_fleetshard_sync_connector_state_count_total` is reported by the cos-fleetshard-sync. Problems with the cos-fleetshard-sync could
greatly affect the accuracy of this metric. Restarting the cos-fleetshard-sync pod will reset the `cos_fleetshard_sync_connector_state_count_total`
counter and possibly tamper with the rate if the period of time used to calculate includes the restart.

More info on this concept of multi-window multi-burn-rate alerts can be found
https://sre.google/workbook/alerting-on-slos/#6-multiwindow-multi-burn-rate-alerts[here].

[WARNING]
SLO LIMIT for Managed Connectors is 99,5%

== Prerequisites

* Access to the OSD cluster via Backplane.
* The ID of the affected cluster. This is provided by the alert via a label. It should look something like this: `c5ff3f4ctr234ihcgr8g`.
* [OPTIONAL] https://github.com/bf2fc6cc711aee1a0c2a/cos-tools/tags[RHOC cli] installed. You have to be part of cos-fleet-manager-admin-full-prod rover group
in order to effectively use the cli.

== Execute/Resolution

. Store the cluster id in a variable for better access
+
----
export CLUSTER_ID=cc6ae6o7764p8lrcfbj0
----

. List connectors to identify which one is failing(STATE=failed).
+
----
% rhoc connectors list --cluster-id ${CLUSTER_ID}

  ID (5)                 NAMESPACE_ID           OWNER                         CONNECTOR_TYPE_ID         DESIRED_STATE   STATE    VERSION   AGE
----------------------- ---------------------- ----------------------------- ------------------------- --------------- -------- --------- --------
  cd74mms89ot9kejrreng   cc6ae6o7764p8lrcfbk0   openbridge_kafka_supporting   slack_sink_0.1            ready           ready    121805    154m
  cd755ss89ot9kejrs13g   cc6ae6o7764p8lrcfbk0   openbridge_kafka_supporting   slack_sink_0.1            ready           ready    121870    122m
  cd76o7takkggo7fhv170   cd75ogc89ot9kejrspng   mk-test-e2e-primary           data_generator_0.1        ready           ready    122013    14m
  cd71vs5akkggo7fhpj8g   cd71tbtakkggo7fhpgmg   yxing@redhat.com              aws_cloudwatch_sink_0.1   ready           failed   121802    5h39m
  cd1ur6dakkggo7fcikbg   cc6ae6o7764p8lrcfbk0   openbridge_kafka_supporting   slack_sink_0.1            ready           ready    121803    7d23h
----

Alternatively you can use oc commands to list connectors and look for pods that are unable to start
----
% oc get pods --all-namespaces | grep mctr
redhat-openshift-connectors-cc6ae6o7764p8lrcfbk0   mctr-cd1ur6bhl69lnuj0urn0-5f59fb5776-5xbpf                        1/1     Running                  0               2d18h
redhat-openshift-connectors-cc6ae6o7764p8lrcfbk0   mctr-cdl60k5ajgdagid3bp00-d69fbdbdd-fvhgk                         0/1     Running                  0               42h
----

Double check with the list of ManagedConnectors. This is because some times not even a Deployment can be created and listing pods wont show a faulty connector.
----
% oc get ManagedConnectors --all-namespaces
NAMESPACE                                          NAME                        CLUSTER_ID             CONNECTOR_ID           CONNECTOR_TYPE_ID   DEPLOYMENT_ID          PHASE     DEPLOYMENT_PHASE
redhat-openshift-connectors-cc6ae6o7764p8lrcfbk0   mctr-cd1ur6bhl69lnuj0urn0   cc6ae6o7764p8lrcfbj0   cd1ur6dakkggo7fcikbg   slack_sink_0.1      cd1ur6bhl69lnuj0urn0   Monitor   ready
redhat-openshift-connectors-cc6ae6o7764p8lrcfbk0   mctr-cdl60k5ajgdagid3bp00   cc6ae6o7764p8lrcfbj0   cdl60jtajgdagid3bov0   http_sink_0.1       cdl60k5ajgdagid3bp00   Monitor   failed
----


. The project where the failing connector is running will have the NAMESPACE_ID attached to the name. For this case `cd71tbtakkggo7fhpgmg`
+
----
oc projects | grep cc6ae6o7764p8lrcfbk0
redhat-openshift-connectors-cc6ae6o7764p8lrcfbk0                  Active

oc project redhat-openshift-connectors-cc6ae6o7764p8lrcfbk0
Now using project "redhat-openshift-connectors-cc6ae6o7764p8lrcfbk0"
----

. List pods in the namespace. A failing connector will most likely restart frequently
or it won't start up at all.
+
----
% oc get pods
NAME                                         READY   STATUS    RESTARTS   AGE
mctr-cd1ur6bhl69lnuj0urn0-7c5d4cd55f-shnvx   1/1     Running   0          3h49m
mctr-cd74mmrhl69lnuj68vag-c5d8dd8c9-tz8rw    1/1     Running   0          3h3m
mctr-cd755t3hl69lnuj69jq0-856f75b99-dmnbq    1/1     Running   0          151m
mctr-cd779jbhl69lnuj6cbc0-7765f5d48c-f7fzm   0/1     Running   0          6m51s
----

. The CONNECTOR_ID is NOT part of the pod name, but it is a label named `cos.bf2.org/connector.id`. Check that you have the right pod
+
----
oc get pod mctr-cd779jbhl69lnuj6cbc0-7765f5d48c-f7fzm -o template --template '{{.metadata.labels}}'
map[camel.apache.org/integration:mctr-cd779jbhl69lnuj6cbc0 cos.bf2.org/connector.id:cd779jbhl69lnuj6cba0 cos.bf2.org/connector.type.id:aws_sqs_source_0.1 cos.bf2.org/deployment.id:cd779jbhl69lnuj6cbc0 cos.bf2.org/operator.type:camel-connector-operator pod-template-hash:7765f5d48c]

----

. Identify the reason why the connector is failing by tailing the pod logs. The connector will fail if external systems can't be reached or accessed. For instance a mysql connector will fail if the mysql server is unreachable or if the provided credentials are wrong.

== Validate

What steps are required to verify that the procedure has been followed correctly and the required changes have been implemented correctly, with the desired outcome.

. Check the alert is no longer firing.
. Check that the connector logs are clean of errors.
. Check the dashboard for `cos_fleetshard_sync_connector_state_count_total{cos_connector_id="<CONNECTOR_ID>" cos_connector_state="failed_but_ready"}` counter.

== Troubleshooting

A connector in CONNECTOR_STATE_FAILED will fail health checks and get restarted:

* Check how many times the connector has been restarted, es example:
+
[source]
----
➜ kubectl get pods strimzi-cluster-operator-6ddcb45f47-2jpp2
NAME                                        READY   STATUS    RESTARTS      AGE
strimzi-cluster-operator-6ddcb45f47-2jpp2   1/1     Running   4 (18m ago)   3d20h
----

* Check events to determine the reason of the restart:
+
[source]
----
➜ kubectl get events
LAST SEEN   TYPE      REASON               OBJECT                                                  MESSAGE
92m         Warning   Unhealthy            pod/cos-fleetshard-operator-debezium-59b9c9bd64-gj44t   Liveness probe failed: Get "http://10.131.0.68:8080/q/health/live": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
80m         Normal    Created              pod/strimzi-cluster-operator-6ddcb45f47-2jpp2           Created container strimzi-cluster-operator
14m         Warning   Unhealthy            pod/strimzi-cluster-operator-6ddcb45f47-2jpp2           Readiness probe failed: Get "http://10.131.0.67:8080/ready": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
29m         Warning   Unhealthy            pod/strimzi-cluster-operator-6ddcb45f47-2jpp2           Liveness probe failed: Get "http://10.131.0.67:8080/healthy": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
80m         Normal    Killing              pod/strimzi-cluster-operator-6ddcb45f47-2jpp2           Container strimzi-cluster-operator failed liveness probe, will be restarted
80m         Normal    Pulled               pod/strimzi-cluster-operator-6ddcb45f47-2jpp2           Container image "quay.io/strimzi/operator:0.28.0" already present on machine
14m         Normal    AllRequirementsMet   clusterserviceversion/strimzi-kafka-operator.v0.28.0    all requirements found, attempting install
14m         Normal    InstallSucceeded     clusterserviceversion/strimzi-kafka-operator.v0.28.0    waiting for install components to report healthy
14m         Normal    InstallWaiting       clusterserviceversion/strimzi-kafka-operator.v0.28.0    installing: waiting for deployment strimzi-cluster-operator to become ready: deployment "strimzi-cluster-operator" not available: Deployment does not have minimum availability.
14m         Normal    InstallSucceeded     clusterserviceversion/strimzi-kafka-operator.v0.28.0    install strategy completed with no errors
14m         Warning   ComponentUnhealthy   clusterserviceversion/strimzi-kafka-operator.v0.28.0    installing: waiting for deployment strimzi-cluster-operator to become ready: deployment "strimzi-cluster-operator" not available: Deployment does not have minimum availability.
14m         Normal    NeedsReinstall       clusterserviceversion/strimzi-kafka-operator.v0.28.0    installing: waiting for deployment strimzi-cluster-operator to become ready: deployment "strimzi-cluster-operator" not available: Deployment does not have minimum availability.
----

* Check the logs:
+
[source]
----
oc logs -f strimzi-cluster-operator-6ddcb45f47-2jpp2
----

Some useful information can be found in the resources managed by the fleetshard operators.

* Inspect ManagedConnector resource:
+
[source]
----
oc get ManagedConnector -l cos.bf2.org/connector.id=<connectorId> -n redhat-openshift-connectors-<namespaceId>
----

* Inspect the fleetshard operator camel
+
[source]
----
oc get pod -l app.kubernetes.io/name=cos-fleetshard-operator-camel
oc logs -l app.kubernetes.io/name=cos-fleetshard-operator-camel -f
----

* Inspect the KameletBinding resource
+
[source]
----
oc get KameletBinding -n redhat-openshift-connectors-<namespaceId>
oc get KameletBinding -l cos.bf2.org/connector.id=<connectorId> -n redhat-openshift-connectors-<namespaceId>
----

* Inspect the camelk operator
+
[source]
----
oc get pod -l app=camel-k
oc logs -l app=camel-k -f
----

* Inspect the Integration resource
+
[source]
----
oc get Integration -n redhat-openshift-connectors-<namespaceId>
oc get Integration -l cos.bf2.org/connector.id=<connectorId> -n redhat-openshift-connectors-<namespaceId>
----

* Inspect the connector Deployment
+
[source]
----
oc get Deployment -l cos.bf2.org/connector.id=<connectorId> -n redhat-openshift-connectors-<namespaceId>
oc get ReplicaSet -l cos.bf2.org/connector.id=<connectorId> -n redhat-openshift-connectors-<namespaceId>
oc get Pod -l cos.bf2.org/connector.id=<connectorId> -n redhat-openshift-connectors-<namespaceId>
oc logs -l cos.bf2.org/connector.id=<connectorId> -n redhat-openshift-connectors-<namespaceId> -f
----

== Escalate to engineering

* If the above hasn't worked contact engineering.
** Use the RHOC Run The Service channel in slack: #rhoc-rts.
** Send a mail to rhoc-rts@redhat.com.