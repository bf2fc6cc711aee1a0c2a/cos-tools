// begin header
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]
:numbered:
:toc: macro
:toc-title: pass:[<b>Table of Contents</b>]
// end header
= Some SOP/Alert Name

toc::[]

== Description

The purpose of this SOP is to describe the process of resolving the alert `ConnectorInFailedState`.

// Include the following step in every alert SOP, changing required parts
The source of the alert is the `cos_fleetshard_sync_connector_state` metric which can have 5 values representing following connector's states:

- CONNECTOR_STATE_READY = 1
- CONNECTOR_STATE_FAILED = 2
- CONNECTOR_STATE_DELETED = 3
- CONNECTOR_STATE_STOPPED = 4
- CONNECTOR_STATE_IN_PROCESS = 5

It fires when a connector's been in CONNECTOR_STATE_FAILED for a period of 30 minutes. In order for this alert to trigger, the failing connector must reach CONNECTOR_STATE_READY before failing meaning the conector was able to run in the past.

== Prerequisites

* Access to the OSD cluster via Backplane. The cluster ID is provided by the alert via a label.
* The ID of the affected connector. This is provided by the alert via a label. It should look something like this: `c5ff3f4ctr234ihcgr8g`.

== Execute/Resolution

. Log in to the OSD cluster that generated the alert

. Identify the reason why the connector is failing by tailing the pod logs. The connector will fail if external systems can't be reached or accessed. For instance a mysql connector will fail if the mysql server is unreachable or if the provided credentials are wrong.

== Validate

What steps are required to verify that the procedure has been followed correctly and the required changes have been implemented correctly, with the desired outcome.

. Check the alert is no longer firing.
. Check that the connector logs are clean of errors.
. Check the dashboard shows `cos_fleetshard_sync_connector_state = 1` for the troubled connector.

== Troubleshooting

A connector in CONNECTOR_STATE_FAILED will fail health checks and get restarted:

* Check how many times the connector has been restarted, es example:
+
[source]
----
➜ kubectl get pods strimzi-cluster-operator-6ddcb45f47-2jpp2
NAME                                        READY   STATUS    RESTARTS      AGE
strimzi-cluster-operator-6ddcb45f47-2jpp2   1/1     Running   4 (18m ago)   3d20h
----

* Check events to determine the reason of the restart:
+
[source]
----
➜ kubectl get events
LAST SEEN   TYPE      REASON               OBJECT                                                  MESSAGE
92m         Warning   Unhealthy            pod/cos-fleetshard-operator-debezium-59b9c9bd64-gj44t   Liveness probe failed: Get "http://10.131.0.68:8080/q/health/live": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
80m         Normal    Created              pod/strimzi-cluster-operator-6ddcb45f47-2jpp2           Created container strimzi-cluster-operator
14m         Warning   Unhealthy            pod/strimzi-cluster-operator-6ddcb45f47-2jpp2           Readiness probe failed: Get "http://10.131.0.67:8080/ready": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
29m         Warning   Unhealthy            pod/strimzi-cluster-operator-6ddcb45f47-2jpp2           Liveness probe failed: Get "http://10.131.0.67:8080/healthy": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
80m         Normal    Killing              pod/strimzi-cluster-operator-6ddcb45f47-2jpp2           Container strimzi-cluster-operator failed liveness probe, will be restarted
80m         Normal    Pulled               pod/strimzi-cluster-operator-6ddcb45f47-2jpp2           Container image "quay.io/strimzi/operator:0.28.0" already present on machine
14m         Normal    AllRequirementsMet   clusterserviceversion/strimzi-kafka-operator.v0.28.0    all requirements found, attempting install
14m         Normal    InstallSucceeded     clusterserviceversion/strimzi-kafka-operator.v0.28.0    waiting for install components to report healthy
14m         Normal    InstallWaiting       clusterserviceversion/strimzi-kafka-operator.v0.28.0    installing: waiting for deployment strimzi-cluster-operator to become ready: deployment "strimzi-cluster-operator" not available: Deployment does not have minimum availability.
14m         Normal    InstallSucceeded     clusterserviceversion/strimzi-kafka-operator.v0.28.0    install strategy completed with no errors
14m         Warning   ComponentUnhealthy   clusterserviceversion/strimzi-kafka-operator.v0.28.0    installing: waiting for deployment strimzi-cluster-operator to become ready: deployment "strimzi-cluster-operator" not available: Deployment does not have minimum availability.
14m         Normal    NeedsReinstall       clusterserviceversion/strimzi-kafka-operator.v0.28.0    installing: waiting for deployment strimzi-cluster-operator to become ready: deployment "strimzi-cluster-operator" not available: Deployment does not have minimum availability.
----

* Check the logs:
+
[source]
----
oc logs -f strimzi-cluster-operator-6ddcb45f47-2jpp2
----

== Further Reading

Put any extra information about the alert here. I.e. Information that isn't critical to resolving to alert but might help SRE who's not familiar with the alert understand it and it's implications if they want to.
